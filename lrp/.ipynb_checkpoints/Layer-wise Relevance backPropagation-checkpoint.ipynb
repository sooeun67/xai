{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6bb0183c8e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# import matplotlib.pyplot as plt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m  \u001b[0;31m# 라이브러리가 기본으로 제공하는 mnist 데이터셋\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m  \u001b[0;31m# one-hot encoding 을 위한 함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m  \u001b[0;31m# 레이어를 층층히 쌓아가는 연쇄 모델\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist  # 라이브러리가 기본으로 제공하는 mnist 데이터셋\n",
    "from tensorflow.keras.utils import to_categorical  # one-hot encoding 을 위한 함수\n",
    "from tensorflow.keras.models import Sequential  # 레이어를 층층히 쌓아가는 연쇄 모델\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import load_model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shape] train: (50000, 28, 28) \n",
      "[shape] test: (10000, 28, 28) \n",
      "[shape] valid: (10000, 28, 28) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fda901f67b8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train_, y_train_), (x_valid_, y_valid_) = mnist.load_data()\n",
    "x_test_, y_test_ = x_train_[50000:], y_train_[50000:]\n",
    "x_train_,y_train_ = x_train_[:50000], y_train_[:50000]\n",
    "\n",
    "print(f'[shape] train: {x_train_.shape} ')\n",
    "print(f'[shape] test: {x_test_.shape} ')\n",
    "print(f'[shape] valid: {x_valid_.shape} ')\n",
    "\n",
    "plt.imshow(x_train_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shape] before train: (50000, 28, 28) \n",
      "[shape] after train: (50000, 784) \n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train_.shape[1]*x_train_.shape[2]\n",
    "class_num = len(set(y_train_))\n",
    "\n",
    "x_train = (x_train_/255.0).reshape(-1, input_shape)\n",
    "x_test = (x_test_/255.0).reshape(-1, input_shape)\n",
    "x_valid = (x_valid_/255.0).reshape(-1, input_shape)\n",
    "\n",
    "print(f'[shape] before train: {x_train_.shape} ')\n",
    "print(f'[shape] after train: {x_train.shape} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[example] before train: 0 \n",
      "[example] after train: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "I = np.eye(10)\n",
    "\n",
    "y_train = I[y_train_]\n",
    "y_test = I[y_test_]\n",
    "y_valid = I[y_valid_]\n",
    "\n",
    "print(f'[example] before train: {y_train_[1]} ')\n",
    "print(f'[example] after train: {y_train[1]} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MNISTLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e096038d4022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MNISTLoader' is not defined"
     ]
    }
   ],
   "source": [
    "dataloader = MNISTLoader()\n",
    "\t\t\n",
    "x_train, y_train = dataloader.train\n",
    "x_validation, y_validation = dataloader.validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Feed-forward network\n",
    "# -------------------------\n",
    "class Network:\n",
    "    def __init__(self,layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self,Z):\n",
    "        for l in self.layers: Z = l.forward(Z)\n",
    "        return Z\n",
    "    \n",
    "    def gradprop(self,DZ):\n",
    "        for l in self.layers[::-1]: DZ = l.gradprop(DZ)\n",
    "        return DZ\n",
    "    \n",
    "    def relprop(self, R):\n",
    "        for I in self.layers[::-1]: R = I.relprop(R)\n",
    "        return R\n",
    "\n",
    "# -------------------------\n",
    "# ReLU activation layer\n",
    "# -------------------------\n",
    "class ReLU:\n",
    "    def forward(self,X):\n",
    "        self.Z = X>0\n",
    "        return X*self.Z\n",
    "    \n",
    "    def gradprop(self,DY):\n",
    "        return DY*self.Z\n",
    "    \n",
    "    def relprop(self, R): \n",
    "        return R\n",
    "    \n",
    "# -------------------------\n",
    "# Fully-connected layer\n",
    "# -------------------------\n",
    "class Linear:\n",
    "\n",
    "    def __init__(self, weight, bias):\n",
    "        self.W = weight\n",
    "        self.B = bias\n",
    "\n",
    "    def forward(self,X):\n",
    "        self.X = X\n",
    "        return np.dot(self.X,self.W)+self.B\n",
    "\n",
    "    def gradprop(self,DY):\n",
    "        self.DY = DY # DY는 target을 넣으면 됨. Desired Y\n",
    "        return np.dot(self.DY,self.W.T)\n",
    "\n",
    "class NextLinear(Linear): # implementing Z+ rule\n",
    "    def relprop(self,R):\n",
    "        V = np.maximum(0,self.W) # V는 W_ij^+를 의미함.\n",
    "        Z = np.dot(self.X,V)+1e-9; S = R/Z\n",
    "        C = np.dot(S,V.T);         R = self.X*C\n",
    "        return R\n",
    "    \n",
    "class FirstLinear(Linear): # implementing Zbeta rule\n",
    "    def relprop(self,R):\n",
    "        W,V,U = self.W,np.maximum(0,self.W),np.minimum(0,self.W)\n",
    "#        X,L,H = self.X,self.X*0+utils.lowest,self.X*0+utils.highest\n",
    "        X,L,H = self.X,self.X*0+(-1),self.X*0+(1.0)\n",
    "\n",
    "\n",
    "        Z = np.dot(X,W)-np.dot(L,V)-np.dot(H,U)+1e-9; S = R/Z\n",
    "        R = X*np.dot(S,W.T)-L*np.dot(S,V.T)-H*np.dot(S,U.T)\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.init_ops_v2.RandomNormal at 0x7fda8f6e5e80>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-4ad40d51a3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mrand_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Network 구조 입력\n",
    "initializer = tf.random_normal_initializer(\n",
    "    mean=.0,\n",
    "    stddev=0.1,\n",
    ")\n",
    "# final_W1 = tf.get_variable(\n",
    "#     shape=[\n",
    "#         3,3,\n",
    "#     ],\n",
    "#     dtype=tf.float32,\n",
    "#     initializer=initializer,\n",
    "# )\n",
    "# final_b1 = tf.get_variable(\n",
    "#     shape=[present_weight],\n",
    "#     dtype=tf.float32,\n",
    "#     initializer=initializer,\n",
    "# )\n",
    "final_W1, final_W2, final_W3, final_W4, final_Wh = [1, 1,3,3,5]\n",
    "final_b1, final_b2, final_b3, final_b4, final_bh = [0.5,0.5,0.3,0.3,0.2]\n",
    "n_total=3\n",
    "total_x=train_x\n",
    "\n",
    "\n",
    "nn = Network([\n",
    "            \n",
    "    FirstLinear(final_W1, final_b1),ReLU(),\n",
    "    NextLinear(final_W2, final_b2),ReLU(),\n",
    "    NextLinear(final_W3, final_b3),ReLU(),\n",
    "    NextLinear(final_W4, final_b4),ReLU(),\n",
    "    NextLinear(final_Wh, final_bh),ReLU(),\n",
    "])\n",
    "\n",
    "rand_num = np.random.permutation(n_total)\n",
    "\n",
    "X = total_x[rand_num,:] # Input\n",
    "T = total_y[rand_num,:] # Target\n",
    "Y = nn.forward(X) # Output\n",
    "S = nn.gradprop(T)**2\n",
    "Y = nn.forward(X)\n",
    "\n",
    "D = nn.relprop(Y*T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_W1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a0950cc964e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Network 구조 입력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m nn = Network([\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mFirstLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_W1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_b1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mNextLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_W2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_b2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mNextLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_W3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_b3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_W1' is not defined"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Feed-forward network\n",
    "# -------------------------\n",
    "class Network:\n",
    "    def __init__(self,layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self,Z):\n",
    "        for l in self.layers: Z = l.forward(Z)\n",
    "        return Z\n",
    "    \n",
    "    def gradprop(self,DZ):\n",
    "        for l in self.layers[::-1]: DZ = l.gradprop(DZ)\n",
    "        return DZ\n",
    "    \n",
    "    def relprop(self, R):\n",
    "        for I in self.layers[::-1]: R = I.relprop(R)\n",
    "        return R\n",
    "\n",
    "# -------------------------\n",
    "# ReLU activation layer\n",
    "# -------------------------\n",
    "class ReLU:\n",
    "    def forward(self,X):\n",
    "        self.Z = X>0\n",
    "        return X*self.Z\n",
    "    \n",
    "    def gradprop(self,DY):\n",
    "        return DY*self.Z\n",
    "    \n",
    "    def relprop(self, R): \n",
    "        return R\n",
    "    \n",
    "# -------------------------\n",
    "# Fully-connected layer\n",
    "# -------------------------\n",
    "class Linear:\n",
    "\n",
    "  def __init__(self, weight, bias):\n",
    "    self.W = weight\n",
    "    self.B = bias\n",
    "\n",
    "  def forward(self,X):\n",
    "    self.X = X\n",
    "    return np.dot(self.X,self.W)+self.B\n",
    "\n",
    "  def gradprop(self,DY):\n",
    "    self.DY = DY # DY는 target을 넣으면 됨. Desired Y\n",
    "    return np.dot(self.DY,self.W.T)\n",
    "\n",
    "\n",
    "class NextLinear(Linear): # implementing Z+ rule\n",
    "    def relprop(self,R):\n",
    "        V = np.maximum(0,self.W) # V는 W_ij^+를 의미함.\n",
    "        Z = np.dot(self.X,V)+1e-9; S = R/Z\n",
    "        C = np.dot(S,V.T);         R = self.X*C\n",
    "        return R\n",
    "    \n",
    "class FirstLinear(Linear): # implementing Zbeta rule\n",
    "    def relprop(self,R):\n",
    "        W,V,U = self.W,np.maximum(0,self.W),np.minimum(0,self.W)\n",
    "#        X,L,H = self.X,self.X*0+utils.lowest,self.X*0+utils.highest\n",
    "        X,L,H = self.X,self.X*0+(-1),self.X*0+(1.0)\n",
    "\n",
    "\n",
    "        Z = np.dot(X,W)-np.dot(L,V)-np.dot(H,U)+1e-9; S = R/Z\n",
    "        R = X*np.dot(S,W.T)-L*np.dot(S,V.T)-H*np.dot(S,U.T)\n",
    "        return R\n",
    "\n",
    "# Network 구조 입력\n",
    "nn = Network([\n",
    "    FirstLinear(final_W1, final_b1),ReLU(),\n",
    "    NextLinear(final_W2, final_b2),ReLU(),\n",
    "    NextLinear(final_W3, final_b3),ReLU(),\n",
    "    NextLinear(final_W4, final_b4),ReLU(),\n",
    "    NextLinear(final_Wh, final_bh),ReLU(),\n",
    "])\n",
    "\n",
    "rand_num = np.random.permutation(n_total)\n",
    "\n",
    "X = total_x[rand_num,:] # Input\n",
    "T = total_y[rand_num,:] # Target\n",
    "Y = nn.forward(X) # Output\n",
    "S = nn.gradprop(T)**2\n",
    "Y = nn.forward(X)\n",
    "\n",
    "D = nn.relprop(Y*T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ef3a69c366fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from models.models_2_3 import MNIST_NN, MNIST_DNN, LRP\n",
    "from utils import pixel_range\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "images = mnist.train.images\n",
    "labels = mnist.train.labels\n",
    "\n",
    "logdir = './tf_logs/2_3_LRP/'\n",
    "ckptdir = logdir + 'model'\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.mkdir(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_dense(activation, kernel, bias, relevance):\n",
    "    W_p = tf.maximum(0., kernel)\n",
    "    b_p = tf.maximum(0., bias)\n",
    "    z_p = tf.matmul(activation, W_p) + b_p\n",
    "    s_p = relevance / z_p\n",
    "    c_p = tf.matmul(s_p, tf.transpose(W_p))\n",
    "\n",
    "    W_n = tf.minimum(0., kernel)\n",
    "    b_n = tf.minimum(0., bias)\n",
    "    z_n = tf.matmul(activation, W_n) + b_n\n",
    "    s_n = relevance / z_n\n",
    "    c_n = tf.matmul(s_n, tf.transpose(W_n))\n",
    "\n",
    "    return activation * (self.alpha * c_p + (1 - self.alpha) * c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, X, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            with tf.variable_scope('layer0'):\n",
    "                X_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "            # Convolutional Layer #1 and Pooling Layer #1\n",
    "            with tf.variable_scope('layer1'):\n",
    "                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu, use_bias=False)\n",
    "                pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
    "\n",
    "            # Convolutional Layer #2 and Pooling Layer #2\n",
    "            with tf.variable_scope('layer2'):\n",
    "                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu, use_bias=False)\n",
    "                pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
    "\n",
    "            # Convolutional Layer #3 and Pooling Layer #3\n",
    "            with tf.variable_scope('layer3'):\n",
    "                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu, use_bias=False)\n",
    "                pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
    "\n",
    "            # Dense Layer with Relu\n",
    "            with tf.variable_scope('layer4'):\n",
    "                flat = tf.reshape(pool3, [-1, 128 * 4 * 4])\n",
    "                dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu, use_bias=False)\n",
    "\n",
    "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
    "            with tf.variable_scope('layer5'):\n",
    "                logits = tf.layers.dense(inputs=dense4, units=10, use_bias=False)\n",
    "                prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        return [X_img, conv1, pool1, conv2, pool2, conv3, pool3, flat, dense4, prediction], logits\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib.layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ffaccc102dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtcl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMNIST_DNN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib.layers'"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.layers as tcl\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MNIST_DNN:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, X, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            dense1 = tf.layers.dense(inputs=X, units=512, activation=tf.nn.relu)\n",
    "            dense2 = tf.layers.dense(inputs=dense1, units=100, activation=tf.nn.relu)\n",
    "            logits = tf.layers.dense(inputs=dense2, units=10)\n",
    "\n",
    "        return dense2, logits\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
    "\n",
    "\n",
    "class MNIST_G(object):\n",
    "\n",
    "    def __init__(self, z_dim, name):\n",
    "        self.z_dim = z_dim\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, z, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            G_W1 = tf.get_variable('G_W1', [self.z_dim, 128], initializer=tcl.xavier_initializer())\n",
    "            G_b1 = tf.get_variable('G_b1', [128], initializer=tf.constant_initializer())\n",
    "            G_W2 = tf.get_variable('G_W2', [128, 784], initializer=tcl.xavier_initializer())\n",
    "            G_b2 = tf.get_variable('G_b2', [784], initializer=tf.constant_initializer())\n",
    "\n",
    "            layer1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "            layer2 = tf.nn.sigmoid(tf.matmul(layer1, G_W2) + G_b2)\n",
    "\n",
    "        return layer2\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
    "\n",
    "\n",
    "class MNIST_D(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, X, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            D_W1 = tf.get_variable('D_W1', [784, 128], initializer=tcl.xavier_initializer())\n",
    "            D_b1 = tf.get_variable('D_b1', [128], initializer=tf.constant_initializer())\n",
    "            D_W2 = tf.get_variable('D_W2', [128, 1], initializer=tcl.xavier_initializer())\n",
    "            D_b2 = tf.get_variable('D_b2', [1], initializer=tf.constant_initializer())\n",
    "\n",
    "            layer1 = tf.nn.relu(tf.matmul(X, D_W1) + D_b1)\n",
    "            layer2 = tf.matmul(layer1, D_W2) + D_b2\n",
    "            prediction = tf.nn.sigmoid(layer2)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-073b3eda7757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Setup training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('Classifier'):\n",
    "\n",
    "    # Initialize neural network\n",
    "    DNN = MNIST_DNN('DNN')\n",
    "\n",
    "    # Setup training process\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name='Y')\n",
    "\n",
    "    activations, logits = DNN(X)\n",
    "    \n",
    "    tf.add_to_collection('LRP', X)\n",
    "    \n",
    "    for activation in activations:\n",
    "        tf.add_to_collection('LRP', activation)\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost, var_list=DNN.vars)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "cost_summary = tf.summary.scalar('Cost', cost)\n",
    "accuray_summary = tf.summary.scalar('Accuracy', accuracy)\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "# Hyper parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    avg_acc = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, a, summary_str = sess.run([optimizer, cost, accuracy, summary], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        avg_cost += c / total_batch\n",
    "        avg_acc += a / total_batch\n",
    "        \n",
    "        file_writer.add_summary(summary_str, epoch * total_batch + i)\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost), 'accuracy =', '{:.9f}'.format(avg_acc))\n",
    "    \n",
    "    saver.save(sess, ckptdir)\n",
    "\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, X, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            with tf.variable_scope('layer0'):\n",
    "                X_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "            # Convolutional Layer #1 and Pooling Layer #1\n",
    "            with tf.variable_scope('layer1'):\n",
    "                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu, use_bias=False)\n",
    "                pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
    "\n",
    "            # Convolutional Layer #2 and Pooling Layer #2\n",
    "            with tf.variable_scope('layer2'):\n",
    "                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu, use_bias=False)\n",
    "                pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
    "\n",
    "            # Convolutional Layer #3 and Pooling Layer #3\n",
    "            with tf.variable_scope('layer3'):\n",
    "                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu, use_bias=False)\n",
    "                pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
    "\n",
    "            # Dense Layer with Relu\n",
    "            with tf.variable_scope('layer4'):\n",
    "                flat = tf.reshape(pool3, [-1, 128 * 4 * 4])\n",
    "                dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu, use_bias=False)\n",
    "\n",
    "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
    "            with tf.variable_scope('layer5'):\n",
    "                logits = tf.layers.dense(inputs=dense4, units=10, use_bias=False)\n",
    "                prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        return [X_img, conv1, pool1, conv2, pool2, conv3, pool3, flat, dense4, prediction], logits\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.ops import nn_ops, gen_nn_ops\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.mstats import zscore\n",
    "\n",
    "#Helper Method for \n",
    "\n",
    "def lrp(F, lowest, highest, graph=None, return_flist=False):\n",
    "    \"\"\"\n",
    "        Accepts a final output, and propagates back from there to compute LRP over a tensorflow graph. \n",
    "        Performs a Taylor Decomp at each layer to assess the relevances of each neuron at that layer\n",
    "    \"\"\"\n",
    "    #Assumptions:\n",
    "        #all conv strides are [1,1,1,1]\n",
    "        #all pool strides are [1,2,2,1]\n",
    "        #all pool/conv padding is SAME\n",
    "        #only reshaping that happens is after a pool layer (pool -> fc) or a conv layer (conv -> fc)\n",
    "    F_list = []\n",
    "    traversed, graph, graph_dict, var_dict = get_traversed(graph=graph)\n",
    "    for n in traversed:\n",
    "        val_name = next(I for I in graph_dict[n].input if I in traversed).split(\"/read\")[0] + \":0\"\n",
    "        X = graph.get_tensor_by_name(val_name)      \n",
    "        if graph_dict[n].op == \"MatMul\":\n",
    "            weight_name = next(I for I in graph_dict[n].input if not I in traversed).split(\"/read\")[0] + \":0\"\n",
    "            W = var_dict[weight_name]\n",
    "            if \"absolute_input\" in graph_dict[n].input:\n",
    "                F = fprop_first(F, W, X, lowest, highest)\n",
    "                F_list.append(F)\n",
    "                break\n",
    "            else:\n",
    "                F = fprop(F, W, X) \n",
    "                F_list.append(F)\n",
    "        elif graph_dict[n].op == \"MaxPool\" or graph_dict[n].op ==  \"MaxPoolWithArgmax\":\n",
    "            F = fprop_pool(F, X)     \n",
    "            F_list.append(F)\n",
    "        elif graph_dict[n].op == \"Conv2D\":\n",
    "            weight_name = next(I for I in graph_dict[n].input if not I in traversed).split(\"/read\")[0] + \":0\"\n",
    "            W = var_dict[weight_name]\n",
    "            if \"absolute_input\" in graph_dict[n].input:\n",
    "                F = fprop_conv_first(F, W, X, lowest, highest)\n",
    "                F_list.append(F)\n",
    "                break\n",
    "            else:\n",
    "                F = fprop_conv(F, W, X) \n",
    "                F_list.append(F)\n",
    "    if return_flist:\n",
    "        return F_list\n",
    "    else:\n",
    "        return F\n",
    "\n",
    "def get_traversed(graph = None):\n",
    "    #Get the graph and graph traversal\n",
    "    graph = tf.get_default_graph() if graph is None else graph\n",
    "    graph_dict = {node.name:node for node in graph.as_graph_def().node}\n",
    "    var_dict = {v.name:v.value() for v in tf.get_collection(tf.GraphKeys.VARIABLES)}\n",
    "    return traverse(graph_dict[\"absolute_output\"], [], graph_dict), graph, graph_dict, var_dict\n",
    "\n",
    "\n",
    "def traverse(node, L, graph_dict):\n",
    "    #Depth First Search the Network Graph\n",
    "    L.append(node.name)\n",
    "    if \"absolute_input\" in node.name:\n",
    "        return L\n",
    "    inputs = node.input\n",
    "    for nodename in inputs:\n",
    "        if not traverse(graph_dict[nodename], L, graph_dict) is None:\n",
    "            return L\n",
    "    return None\n",
    "\n",
    "def fprop_first(F, W, X, lowest, highest):\n",
    "    #Propagate from last feedforward layer to input\n",
    "    W,V,U = W,tf.maximum(0.0,W), tf.minimum(0.0,W)\n",
    "    X,L,H = X, X*0+lowest, X*0+highest\n",
    "\n",
    "    Z = tf.matmul(X, W)-tf.matmul(L, V)-tf.matmul(H, U)+1e-9\n",
    "    S = F/Z\n",
    "    F = X*tf.matmul(S,tf.transpose(W))-L*tf.matmul(S, tf.transpose(V))-H*tf.matmul(S,tf.transpose(U))\n",
    "    return F\n",
    "\n",
    "def fprop(F, W, X):\n",
    "    #Propagate over feedforward layer\n",
    "    V = tf.maximum(0.0, W)\n",
    "    Z = tf.matmul(X, V)+1e-9;\n",
    "    S = F/Z\n",
    "    C = tf.matmul(S, tf.transpose(V))        \n",
    "    F = X*C\n",
    "    return F\n",
    "\n",
    "def fprop_conv_first(F, W, X, lowest, highest, strides=None, padding='SAME'):\n",
    "    #Propagate from last conv layer to input\n",
    "    strides = [1, 1, 1, 1] if strides is None else strides\n",
    "\n",
    "    Wn = tf.minimum(0.0, W)\n",
    "    Wp = tf.maximum(0.0, W)\n",
    "\n",
    "    X, L, H = X, X*0+lowest, X*0+highest\n",
    "\n",
    "    c  = tf.nn.conv2d(X, W, strides, padding)\n",
    "    cp = tf.nn.conv2d(H, Wp, strides, padding)\n",
    "    cn = tf.nn.conv2d(L, Wn, strides, padding)\n",
    "    Z = c - cp - cn + 1e-9\n",
    "    S = F/Z\n",
    "    \n",
    "    g  = nn_ops.conv2d_backprop_input(tf.shape(X), W,  S, strides, padding)\n",
    "    gp = nn_ops.conv2d_backprop_input(tf.shape(X), Wp, S, strides, padding)\n",
    "    gn = nn_ops.conv2d_backprop_input(tf.shape(X), Wn, S, strides, padding)\n",
    "    F = X*g - L*gp - H*gn\n",
    "    return F\n",
    "\n",
    "def fprop_conv(F, W, X, strides=None, padding='SAME'):\n",
    "    #Propagate over conv layer\n",
    "    xshape = X.get_shape().as_list()\n",
    "    fshape = F.get_shape().as_list()\n",
    "    if len(xshape) != len(fshape):\n",
    "        F = tf.reshape(F, (-1, xshape[1], xshape[2], fshape[-1]/(xshape[1]*xshape[2])))\n",
    "    strides = [1, 1, 1, 1] if strides is None else strides\n",
    "    W = tf.maximum(0.0, W)\n",
    "\n",
    "    Z = tf.nn.conv2d(X, W, strides, padding) + 1e-9 \n",
    "    S = F/Z\n",
    "    C = nn_ops.conv2d_backprop_input(tf.shape(X), W,  S, strides, padding)\n",
    "    F = X*C\n",
    "    return F\n",
    "\n",
    "def fprop_pool(F, X, strides=None, ksize=None, padding='SAME'):\n",
    "    #Propagate over pool layer\n",
    "    xshape = X.get_shape().as_list()\n",
    "    fshape = F.get_shape().as_list()\n",
    "    if len(xshape) != len(fshape):\n",
    "        F = tf.reshape(F, (-1, int(np.ceil(xshape[1]/2.0)), \n",
    "                               int(np.ceil(xshape[2]/2.0)), xshape[3]))\n",
    "    ksize = [1, 2, 2, 1]  if ksize is None else ksize\n",
    "    strides = [1, 2, 2, 1]  if strides is None else strides\n",
    "\n",
    "    Z = tf.nn.max_pool(X, strides=strides, ksize=ksize, padding=padding) + 1e-9\n",
    "    S = F / Z\n",
    "    C = gen_nn_ops._max_pool_grad(X, Z, S, ksize, strides, padding)    \n",
    "    F = X*C\n",
    "    return F\n",
    "\n",
    "\n",
    "def get_lrp_im(sess, F, x, y, xval, yval):\n",
    "    #Compute LRP over the values and labels\n",
    "    im = []\n",
    "    for i in range(0, xval.shape[0]):\n",
    "        im += list(F.eval(session=sess, feed_dict={x: xval[i:i+1], y: yval[i:i+1]}))\n",
    "    return im\n",
    "\n",
    "def visualize(im_list, xval):\n",
    "    #Visualize the LRPs\n",
    "    for i in range(len(im_list[0])):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,1+len(im_list),1)\n",
    "        plt.title(\"Image\")\n",
    "        plt.imshow(xval[i])\n",
    "        \n",
    "        for j in range(len(im_list)):\n",
    "            plt.subplot(1,1+len(im_list),2+j)\n",
    "            plt.title(\"LRP for network {}\".format(j))\n",
    "            I = np.mean(np.maximum(im_list[j][i], 0), -1)\n",
    "            I = np.minimum(I, np.percentile(I, 99))\n",
    "            I = I/np.max(I)\n",
    "            print \"np.linalg.norm(I)\", np.linalg.norm(I)\n",
    "            plt.imshow(I, cmap=\"gray\")\n",
    "\n",
    "        plt.show()\n",
    "    return im_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_1",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
