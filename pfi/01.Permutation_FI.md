
# Permutation Feature Importance (PFI)
<b>Model-Agnostic-Methods</b> : Permutation feature importance is a model inspection technique that can be used for any fitted estimator when the data is tabular. 
This is especially useful for non-linear or opaque estimators. 

> - PFI is defined to be the decrease in a model score when a single feature value is randomly shuffled. 
> - This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. 
> - This technique benefits from being model agnostic and can be calculated many times with different permutations of the feature.

```
Note : Features that are deemed of low importance for a bad model could be very important for a good model. 
Therefore it is always important to evaluate the predictive power of a model prior to computing importances.
Permutation importance does not reflect to the intrinsic predictive value of a feature by itself 
but how important this feature is for a particular model.
```

---

## Calculation
> - Îç∞Ïù¥ÌÑ∞ ÌñâÎ†¨ ùëã(= ùëõ * ùëù)Î°ú ÏÇ¨Ï†ÑÌïôÏäµÎêú Î™®Îç∏ ùëì Í∞Ä ÏûàÏùÑ Îïå (Îç∞Ïù¥ÌÑ∞ nÍ∞ú, ÌäπÏÑ±Ïπò pÍ∞ú) 
> - Permutation, j: ÌôïÏù∏ÌïòÍ≥†Ïûê ÌïòÎäî ÌäπÏÑ±Ïπò(jÏó¥)ÏùÑ ÏàúÏÑúÎßåÏùÑ shuffleÌïú ÏÉà Îç∞Ïù¥ÌÑ∞ ÌñâÎ†¨ ùëã ùëùùëíùëüùëö ÏÉùÏÑ±
> - Base ÏÑ±Îä•Í≥ºÏùò Ï∞®Ïù¥Î•º Feature importance(ùêπùêº(ùëó))Î°ú ÏÇ¨Ïö©. (ùëó = 1,2, . . , ùëù Ïóê ÎåÄÌï¥ Í∞úÎ≥Ñ ÏãúÌñâ)
> - ex) "Person A"Î•º Î∫êÎçîÎãà ÏùºÏù¥ Ïïà ÎèåÏïÑÍ∞ÄÎäîÍµ¨ÎÇò..Person A Í∞Ä ÏñºÎßàÎÇò Ï§ëÏöîÌïúÍ∞Ä

> <img src="images/FI_Shuffle.png" alt="drawing" style="width:600px;"/>

---

## Permutation Importance vs Mean Decrease in Impurity (MDI)
The impurity-based feature importance ranks the numerical features to be the most important features. 
As a result, the non-predictive random_num variable is ranked the most important!

> This problem stems from two limitations of impurity-based feature importances:
> - impurity-based importances are biased towards high cardinality features;
> - impurity-based importances are computed on training set statistics 
>   and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity).

### Examples : <titanic Data> - Extra Tree Model
> Feature Importance - Random variables occupied very high position. Due to two limitations, described above
> This example shows how to use Permutation Importances as an alternative that can mitigate those limitations.

><img src="images/FI_Example.png" alt="drawing" style="width:600px;"/>
  
---
  
## Advantages and Dis-advantages of Permutation Importance
### Advantages
> 1. ÏßÅÍ¥ÄÏ†ÅÏù∏ Ìï¥ÏÑùÏù¥ Í∞ÄÎä•ÌïòÍ≥† Í∏∞Ï°¥ Feature ImportanceÏùò Ï†úÌïúÏ†ê Í∑πÎ≥µ (Random Î≥ÄÏàòÎ•º Ï∞∏Í≥†ÏπòÎ°ú ÏÇ¨Ïö©Í∞ÄÎä•)
> 2. Model-Agnostic Î∞©Î≤ïÏúºÎ°ú Ïñ¥Îñ§ Î™®Îç∏Ïù¥Îì† Ï†ÅÏö©Ïù¥ Í∞ÄÎä•Ìï®
> 3. Test SampleÏóê ÎåÄÌïú Ï†ÅÏö©Ïù¥ Í∞ÄÎä•
  
### Dis-advantages
> 1. Low Calculation Performance compared with FI (MDI)
> 2. Still global interpretation (No local interpretation)
> 3. Still No Y impact direction indications 


## References:
[1] L. Breiman, ‚ÄúRandom Forests‚Äù, Machine Learning, 45(1), 5-32, https://doi.org/10.1023/A:1010933404324
  
[2] https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html
  
[3] https://scikit-learn.org/stable/modules/permutation_importance.html
